---
title: "Research"
layout: default
excerpt: "RT2 Lab -- Research"
sitemap: false
permalink: /research/
---

# Research

For 2026, we have four broad initiatives that capture the breadth of our work.

**Deployment as a Science:** How do people use AI systems around the world?  
Fabric is a public catalog of AI systems deployed in real-world contexts. We capture how individual and institutional oversight help integrate AI into existing workflows. We are building a community of deployers to share best practices and patterns of AI use. We use field experiments to understand deployment strategies, such as when selective access to AI agents improves outcomes and for whom.

**Externalities of Human-AI Interaction:** What is the impact of AI use?  
We are building a large-scale, multi-domain study and open-source platform to track the longitudinal effects of AI use on productivity, confidence, and equity. In parallel, we conduct focused evaluations within single domains and cross-cultural settings to understand heterogeneous impacts of using AI assistance. We investigate moral development by studying how short-story reading, with and without AI mediation, shapes moral acquisition and downstream behavior.

**Affordances of AI Use:** How do people interact with AI systems?  
We examine how the interaction medium, such as speech, text, or multimodal interfaces, changes user cognition, behavior, and performance. We collect in-situ data on how users engage with AI in physical spaces (e.g., art galleries) to understand context-dependent behavior. We are tracking how workplaces evolve as AI is adopted, including risks like misaligned incentives and failure modes reminiscent of the Peter Principle. We seek to build AI thought partners and investigate how LLM use surfaces new forms of harm and bias.

**Agent Orchestration:** How do we deploy AI agents among humans?  
Modiste is our platform to learn policies for when the use of AI agents is appropriate, given user state, task requirements, and organizational constraints. We modulate access to agents by combining dynamic prices, richer objective functions, and purposeful frictions so that assistance aligns with safety, equity, and cost goals. We are extending this to treat humans as high-value information sources, designing protocols for agents to acquire information to improve performance and strengthen oversight.

**Fundamentals of Trustworthy AI**  
Bridging all of these new initiatives is our fundamental technical research into trustworthy AI, spanning transparency, collaboration, and evaluation. Our older research showed traditional explainability techniques have had limited value in real-world deployments. In response, we developed uncertainty-aware explanations and adaptive methods to support users in practice. We went on to design human-AI collaboration mechanisms to improve user performance, incorporate algorithmic resignation when appropriate, and introduce frictions to reduce overreliance and prevent skill atrophy. Our interactive evaluation of LLMs in theorem proving uncovered failure modes invisible to benchmarks. We then aligned models via soft label collection, latent factor selection, and stakeholder-informed hyperparameter tuning. We have ongoing work in all of these areas and are actively seeking strong candidates for our degree programs in this space.
